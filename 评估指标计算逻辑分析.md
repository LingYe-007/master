# HR@K 和 NDCG@K 评估指标计算逻辑分析

## 一、论文中评估指标的描述

### 1.1 指标定义

根据论文第二章（`chapters/chap2_related.tex`）的说明：

#### **HR@K（命中率，Hit Ratio@K）**

**定义**：
$$\text{Hit}@K = \frac{1}{|\mathcal{U}|} \sum_{u \in \mathcal{U}} \mathbb{I}(|\mathcal{R}_u \cap \mathcal{P}_u@K| \geq 1)$$

**含义**：
- $\mathcal{R}_u$：用户 $u$ 的真实正样本集合（Ground Truth）
- $\mathcal{P}_u@K$：模型为用户 $u$ 推荐的前 $K$ 个项目列表
- $\mathbb{I}(\cdot)$：指示函数，条件成立时为1，否则为0

**计算逻辑**：
- 对于每个用户，检查推荐列表Top-K中是否**至少包含一个**用户实际交互过的正样本
- 如果包含至少一个正样本，该用户记1分；否则记0分
- 对所有用户的得分取平均

**特点**：
- 论文中提到"该指标通常用于**留一法（Leave-One-Out）**评估场景中"
- 关注推荐结果的"存在性"，即模型是否"击中"了用户的兴趣点

---

#### **NDCG@K（归一化折损累计增益）**

**定义**：
$$\text{NDCG}@K = \frac{1}{|\mathcal{U}|} \sum_{u \in \mathcal{U}} \frac{\text{DCG}_u@K}{\text{IDCG}_u@K}$$

**其中**：
- $\text{DCG}_u@K = \sum_{i=1}^{K} \frac{2^{rel_i} - 1}{\log_2(i+1)}$
- $rel_i$：第 $i$ 个位置的项目的相关性（隐式反馈中，交互为1，否则为0）
- $\text{IDCG}_u@K$：理想状态下的最大累计增益（用于归一化）

**含义**：
- 不仅关注是否推荐了正确的物品，还关注物品在列表中的**位置**
- 排名越靠前的项目对最终分数的贡献越大（位置折损）
- 值介于0到1之间，值越高代表排序效果越好

---

### 1.2 实验设置

根据第三章（`chapters/chap3_method.tex`）的说明：

- **评估指标**：HR@K 和 NDCG@K
- **K值设置**：$K$ 分别设定为 **10** 和 **20**
- **用途**：
  - K=10：衡量较短推荐列表的召回准确性与排序质量
  - K=20：衡量较长推荐列表的召回准确性与排序质量

---

## 二、评估协议分析

### 2.1 论文中的描述

**⚠️ 重要发现：论文中**没有明确说明**使用了哪种评估协议！**

论文中只提到了：
- 指标的计算公式
- HR@K通常用于"留一法（Leave-One-Out）"评估场景
- 但没有说明：
  - 是否使用**全排序**（对所有物品排序）？
  - 还是使用**采样排序**（如rank-100方案）？

### 2.2 常见的评估协议

在推荐系统评估中，通常有两种评估协议：

#### **协议1：全排序（Full Ranking）**

**流程**：
1. 对于测试集中的每个用户 $u$，计算其对**所有物品**的预测分数
2. 对所有物品按预测分数排序
3. 取Top-K计算HR@K和NDCG@K

**优点**：
- 最接近真实场景（用户看到所有物品）
- 评估结果最可靠

**缺点**：
- 计算开销大（需要计算所有用户-物品对的分数）
- 对于大规模物品集合不现实

---

#### **协议2：采样排序（Sampled Ranking）**

**流程（以rank-100为例）**：
1. 对于测试集中的每个正样本 $(u, i)$（用户 $u$ 交互过的物品 $i$）
2. 从所有未交互的物品中**随机采样100个**作为负样本
3. 将这100个负样本 + 1个正样本（共101个物品）组成候选集
4. 对候选集中的101个物品按预测分数排序
5. 计算HR@K和NDCG@K（K通常≤10或20）

**优点**：
- 计算效率高（只需对101个物品排序）
- 广泛使用，便于与其他方法对比

**缺点**：
- 评估结果可能被高估（因为只与100个负样本竞争）
- 需要多次随机采样取平均以提高可靠性

**常见的采样数量**：
- rank-100：采样100个负样本（最常见）
- rank-50：采样50个负样本
- rank-200：采样200个负样本

---

### 2.3 论文中可能的评估协议

根据论文的描述：

1. **引用LightGCN**（`\cite{he2020lightgcn}`）：
   - LightGCN通常使用**rank-100**的评估协议
   - 这可能暗示论文也使用了类似的协议

2. **HR@K用于"留一法"评估**：
   - 留一法通常配合采样排序使用
   - 每次留出一个正样本进行测试

3. **实验规模**：
   - MovieLens-1M：3,706个物品
   - Yelp：12,095个物品
   - DBLP：18,432个物品
   - 使用全排序计算开销较大，更可能使用采样排序

**推测**：论文很可能使用了**rank-100**的评估协议，但没有在正文中明确说明。

---

## 三、rank-100方案的计算逻辑

### 3.1 评估流程

假设使用rank-100协议：

```
对于测试集中的每个用户 u：
  对于用户 u 的每个测试正样本 (u, i)：
    1. 从所有未交互的物品中随机采样100个作为负样本
    2. 候选集 = {正样本 i} ∪ {100个负样本}  (共101个物品)
    3. 计算用户 u 对候选集中每个物品的预测分数
    4. 按预测分数降序排序
    5. 计算该测试样本的HR@K和NDCG@K
    
对所有测试样本的结果取平均
```

### 3.2 HR@K计算（rank-100场景）

**对于单个测试样本**：
- 如果正样本 $i$ 在排序后的Top-K列表中：HR@K = 1
- 否则：HR@K = 0

**对所有测试样本取平均**：
$$\text{HR}@K = \frac{1}{|\mathcal{T}|} \sum_{(u,i) \in \mathcal{T}} \mathbb{I}(\text{rank}(i) \leq K)$$

其中：
- $\mathcal{T}$：测试集（所有正样本）
- $\text{rank}(i)$：正样本 $i$ 在101个候选物品中的排名

---

### 3.3 NDCG@K计算（rank-100场景）

**对于单个测试样本**：

1. **DCG计算**：
   $$\text{DCG}@K = \sum_{pos=1}^{K} \frac{2^{rel_{pos}} - 1}{\log_2(pos+1)}$$
   
   其中：
   - $rel_{pos}$：位置 $pos$ 的物品的相关性
     - 如果该位置是正样本 $i$：$rel_{pos} = 1$
     - 如果是负样本：$rel_{pos} = 0$

2. **IDCG计算**：
   - 理想情况下，正样本应该排在第一位
   - $\text{IDCG}@K = \frac{2^1 - 1}{\log_2(1+1)} = \frac{1}{\log_2(2)} = 1$

3. **NDCG计算**：
   $$\text{NDCG}@K = \frac{\text{DCG}@K}{\text{IDCG}@K} = \frac{\text{DCG}@K}{1} = \text{DCG}@K$$

**特殊情况**：
- 如果正样本在Top-K之外：$\text{NDCG}@K = 0$
- 如果正样本排在第 $r$ 位（$r \leq K$）：
  $$\text{NDCG}@K = \frac{1}{\log_2(r+1)}$$
  
  例如：
  - 正样本排第1位：NDCG@10 = 1.0
  - 正样本排第2位：NDCG@10 = 1/log₂(3) ≈ 0.631
  - 正样本排第10位：NDCG@10 = 1/log₂(11) ≈ 0.289

---

## 四、确认方法

### 4.1 代码检查

要确认实际使用的评估协议，需要查看代码：

**查找关键词**：
- `rank-100` 或 `rank100`
- `negative sampling` 或 `sample_negative`
- `candidate` 或 `candidate items`
- `evaluation` 或 `evaluate` 函数

**代码位置**：
- 评估脚本（`eval.py` 或 `evaluate.py`）
- 训练脚本中的评估部分
- 数据集加载脚本

**典型代码模式**：

```python
# rank-100 评估协议（示例）
def evaluate_rank100(model, test_data, item_num):
    HR, NDCG = [], []
    for user, pos_item in test_data:
        # 采样100个负样本
        neg_items = np.random.choice(
            range(item_num), 
            size=100, 
            replace=False
        )
        neg_items = neg_items[neg_items != pos_item]  # 排除正样本
        
        # 候选集
        candidates = [pos_item] + neg_items.tolist()
        
        # 计算分数并排序
        scores = model.predict(user, candidates)
        ranked_items = [candidates[i] for i in np.argsort(scores)[::-1]]
        
        # 计算指标
        rank = ranked_items.index(pos_item) + 1
        HR.append(1 if rank <= K else 0)
        NDCG.append(1 / np.log2(rank + 1) if rank <= K else 0)
    
    return np.mean(HR), np.mean(NDCG)
```

---

### 4.2 数值特征判断

根据实验结果数值，可以间接判断：

**如果使用rank-100**：
- NDCG@10的值通常较高（因为只与100个负样本竞争）
- 例如：NDCG@10 ≈ 0.0465（MovieLens-1M）是合理的

**如果使用全排序**：
- NDCG@10的值通常较低（因为要与所有物品竞争）
- 对于MovieLens-1M（3,706个物品），全排序下的NDCG@10会更低

---

## 五、建议

### 5.1 在论文中补充说明

**建议在实验设置部分添加**：

```latex
\subsubsection{评估协议}
本实验采用留一法（Leave-One-Out）评估协议。对于测试集中的每个正样本 $(u, i)$，我们从所有未交互的物品中随机采样100个作为负样本，与正样本一起组成候选集（共101个物品）。然后对候选集中的物品按预测分数排序，计算HR@K和NDCG@K指标。为了确保评估结果的可靠性，我们对每个测试样本重复采样10次并取平均。
```

### 5.2 代码检查清单

- [ ] 查看评估代码，确认是否使用rank-100
- [ ] 确认负样本采样数量（100？50？200？）
- [ ] 确认是否多次采样取平均
- [ ] 确认K值设置（10和20）
- [ ] 对比LightGNN等基线方法的评估协议，确保一致

---

## 六、总结

1. **论文现状**：
   - 定义了HR@K和NDCG@K的计算公式
   - 但没有明确说明评估协议（全排序 vs 采样排序）
   - 提到HR@K用于"留一法"评估

2. **推测**：
   - 很可能使用了**rank-100**的评估协议
   - 基于引用LightGCN和实验规模判断

3. **需要确认**：
   - 查看实际代码中的评估逻辑
   - 在论文中补充评估协议的详细说明
   - 确保与基线方法的评估协议一致

4. **建议**：
   - 在实验设置部分明确说明评估协议
   - 如果使用rank-100，说明采样数量和重复次数
   - 确保评估协议与对比的基线方法一致



